{"meta":{"title":"Ocean's blog","subtitle":"我的目标是星辰大海","description":"大数据技术博客","author":"Ocean","url":"http://yoursite.com","root":"/"},"pages":[{"title":"about","date":"2019-04-22T03:25:15.000Z","updated":"2019-04-22T03:25:30.478Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-04-22T03:23:47.000Z","updated":"2019-04-25T09:46:56.547Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2019-04-22T03:02:02.000Z","updated":"2019-04-25T09:46:49.947Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Spark RDD详解","slug":"Spark-RDD详解","date":"2019-04-26T09:09:03.000Z","updated":"2019-04-26T09:51:31.481Z","comments":true,"path":"2019/04/26/Spark-RDD详解/","link":"","permalink":"http://yoursite.com/2019/04/26/Spark-RDD详解/","excerpt":"概念&emsp;&emsp;RDD(Resilient Distributed Dateset)，弹性分布式数据集。 &emsp;&emsp;RDD是Spark的最基本抽象,是对分布式内存的抽象使用，实现了以操作本地集合的方式来操作分布式数据集的抽象实现。RDD是Spark最核心的东西，它表示已被分区，不可变的并能够被并行操作的数据集合，不同的数据集格式对应不同的RDD实现。RDD必须是可序列化的。RDD可以cache到内存中，每次对RDD数据集的操作之后的结果，都可以存放到内存中，下一个操作可以直接从内存中输入，省去了MapReduce大量的磁盘IO操作。这对于迭代运算比较常见的机器学习算法, 交互式数据挖掘来说，效率提升非常大。 &emsp;&emsp;RDD 最适合那种在数据集上的所有元素都执行相同操作的批处理式应用。在这种情况下， RDD 只需记录血统中每个转换就能还原丢失的数据分区，而无需记录大量的数据操作日志。所以 RDD 不适合那些需要异步、细粒度更新状态的应用 ，比如 Web 应用的存储系统，或增量式的 Web 爬虫等。对于这些应用，使用具有事务更新日志和数据检查点的数据库系统更为高效。","text":"概念&emsp;&emsp;RDD(Resilient Distributed Dateset)，弹性分布式数据集。 &emsp;&emsp;RDD是Spark的最基本抽象,是对分布式内存的抽象使用，实现了以操作本地集合的方式来操作分布式数据集的抽象实现。RDD是Spark最核心的东西，它表示已被分区，不可变的并能够被并行操作的数据集合，不同的数据集格式对应不同的RDD实现。RDD必须是可序列化的。RDD可以cache到内存中，每次对RDD数据集的操作之后的结果，都可以存放到内存中，下一个操作可以直接从内存中输入，省去了MapReduce大量的磁盘IO操作。这对于迭代运算比较常见的机器学习算法, 交互式数据挖掘来说，效率提升非常大。 &emsp;&emsp;RDD 最适合那种在数据集上的所有元素都执行相同操作的批处理式应用。在这种情况下， RDD 只需记录血统中每个转换就能还原丢失的数据分区，而无需记录大量的数据操作日志。所以 RDD 不适合那些需要异步、细粒度更新状态的应用 ，比如 Web 应用的存储系统，或增量式的 Web 爬虫等。对于这些应用，使用具有事务更新日志和数据检查点的数据库系统更为高效。 五大特性 RDD是由一系列的partition组成的。 函数是作用在每一个partition上的。 RDD之间有一系列的依赖关系。 分区器是作用在K,V格式的RDD上。 RDD提供一系列最佳的计算位置。 理解图 血统（Lineage） 特点 textFile方法底层封装的是MR读取文件的方式，在读取文件之前先split，默认split大小是一个block大小。 RDD实际上是不存数据的。 K,V格式的RDD是指RDD内的数据是二元组对象。 如何体现RDD的弹性（容错）？ partition数量、大小没有限制。 RDD之间有依赖关系，可以基于上一个RDD重新计算。 如何体现RDD的分布式？ RDD是由Partition组成的，而partition分布在不同的节点上。 RDD提供了计算最佳位置，体现了数据本地化，符合大数据中“计算移动数据不移动”的理念。 来源：一种是从持久存储获取数据，一种是从其他RDD生成。 基础数据类型&emsp;&emsp;目前有两种类型的基础RDD：并行集合（Parallelized Collections）：接收一个已经存在的Scala集合，然后进行各种并行计算。 Hadoop数据集（Hadoop Datasets）：在一个文件的每条记录上运行函数。只要文件系统是HDFS，或者hadoop支持的任意存储系统即可。这两种类型的RDD都可以通过相同的方式进行操作，从而获得子RDD等一系列拓展，形成lineage血统关系图。 并行化集合 并行化集合是通过调用SparkContext的parallelize方法，在一个已经存在的Scala集合上创建的（一个Seq对象）。集合的对象将会被拷贝，创建出一个可以被并行操作的分布式数据集。例如，下面的解释器输出，演示了如何从一个数组创建一个并行集合。 例如：val rdd = sc.parallelize(Array(1 to 10)) 根据能启动的executor的数量来进行切分多个slice，每一个slice启动一个Task来进行处理。 val rdd = sc.parallelize(Array(1 to 10), 5) 指定了partition的数量。 Hadoop数据集 Spark可以将任何Hadoop所支持的存储资源转化成RDD,如本地文件（需要网络文件系统，所有的节点都必须能访问到）、HDFS、Cassandra、HBase、Amazon S3等，Spark支持文本文件、SequenceFiles和任何Hadoop InputFormat格式。 使用textFile()方法可以将本地文件或HDFS文件转换成RDD，支持整个文件目录读取，文件可以是文本或者压缩文件(如gzip等，自动执行解压缩并加载数据)。 123val rdd1 = sc.textFile(\"file:///root/access_log/access_log\\*.filter\");val rdd2=rdd1.map(_.split(\"t\")).filter(_.length==6)；rdd2.count()； textFile()可选第二个参数slice，默认情况下为每一个block分配一个slice。用户也可以通过slice指定更多的分片，但不能使用少于HDFS block的分片数。 使用wholeTextFiles()读取目录里面的小文件，返回（用户名、内容）对. 使用sequenceFile[K,V]()方法可以将SequenceFile转换成RDD。SequenceFile文件是Hadoop用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。 使用SparkContext.hadoopRDD方法可以将其他任何Hadoop输入类型转化成RDD使用方法。一般来说，HadoopRDD中每一个HDFS block都成为一个RDD分区。 此外，通过Transformation可以将HadoopRDD等转换成FilterRDD(依赖一个父RDD产生）和JoinedRDD（依赖所有父RDD）等。 转换与操作&emsp;&emsp;对于RDD可以有两种计算方式：转换（返回值还是一个RDD）与操作（返回值不是一个RDD） 转换(Transformations) (如：map, filter, groupBy, join等)，Transformations操作是Lazy的，也就是说从一个RDD转换生成另一个RDD的操作不是马上执行，Spark在遇到Transformations操作时只会记录需要这样的操作，并不会去执行，需要等到有Actions操作的时候才会真正启动计算过程进行计算。 操作(Actions) (如：count, collect, save等)，Actions操作会返回结果或把RDD数据写到存储系统中。Actions是触发Spark启动计算的动因。 依赖关系&emsp;&emsp;RDD之间存在一系列的依赖关系，依赖关系分为窄依赖和宽依赖。 窄依赖：父 RDD 和子 RDD partition 之间的关系是一对一的。或者父 RDD 一个partition 只对应一个子 RDD 的 partition 情况下的父 RDD 和子 RDD partition 关系是多对一的。不会有 shuffle 的产生。 宽依赖：父RDD与子RDD partition之间的关系是一对多。会有shuffle的产生。 RDD缓存&emsp;&emsp;RDD的持久化方式有三种：cache、persist、checkpoint。持久化的单位都是partition。其中，cache和persisit都是懒执行的，必须有一个action算子触发执行。checkpoint算子不仅能将RDD持久化到磁盘，还能切断RDD之间的依赖关系。 cache默认将RDD持久化到内存中。cache()是persist()的特例。 persist可以指定持久化的级别。最常用的就是MEMORY_ONLY和MEMORY_AND_DISK.”_2“表示有副本数。具体的持久化级别如下： checkpoint将RDD持久化到磁盘，还可以切断RDD之间的依赖关系。执行原理： 当 RDD 的 job 执行完毕后，会从 finalRDD 从后往前回溯。 当回溯到某一个 RDD 调用了 checkpoint 方法，会对当前的RDD 做一个标记。 Spark 框架会自动启动一个新的 job，重新计算这个 RDD 的数据，将数据持久化到 HDFS 上。 注意事项： cache 和 persist 都是懒执行，必须有一个 action 类算子触发执行。 cache 和 persist 算子的返回值可以赋值给一个变量，在其他 job 中直接使用这个变量就是使用持久化的数据了。持久化的单位是 partition。 cache 和 persist 算子后不能立即紧跟 action 算子。 对 RDD 执行 checkpoint 之前，最好对这个 RDD 先执行cache，这样新启动的 job 只需要将内存中的数据拷贝到 HDFS上就可以，省去了重新计算这一步。","categories":[{"name":"学习","slug":"学习","permalink":"http://yoursite.com/categories/学习/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"},{"name":"RDD","slug":"RDD","permalink":"http://yoursite.com/tags/RDD/"}]},{"title":"Spark编程模型","slug":"Spark编程模型","date":"2019-04-26T08:11:43.000Z","updated":"2019-04-26T09:09:44.219Z","comments":true,"path":"2019/04/26/Spark编程模型/","link":"","permalink":"http://yoursite.com/2019/04/26/Spark编程模型/","excerpt":"&emsp;&emsp;Spark和MapReduce都是分布式计算框架，但是Spark处理数据的能力是MR的十倍以上，不仅仅是因为Spark基于内存，MR基于HDFS，还有就是Spark通过DAG有向无环图来切分任务的执行先后顺序。","text":"&emsp;&emsp;Spark和MapReduce都是分布式计算框架，但是Spark处理数据的能力是MR的十倍以上，不仅仅是因为Spark基于内存，MR基于HDFS，还有就是Spark通过DAG有向无环图来切分任务的执行先后顺序。 概念 应用程序（Application）： 基于Spark的用户程序，包含了一个Driver Program 和集群中多个的Executor； 驱动程序（Driver Program）：运行Application的main()函数并且创建SparkContext，通常用SparkContext代表Driver Program； 执行单元（Executor）： 是为某Application运行在Worker Node上的一个进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上，每个Application都有各自独立的Executors； 集群管理程序（Cluster Manager）： 在集群上获取资源的外部服务(例如：Standalone、Mesos或Yarn)； 操作（Operation）：作用于RDD的各种操作分为Transformation和Action； 任务执行原理&emsp;&emsp;Driver和Worker是启动在节点上的进程，运行在JVM中的进程。 Driver 与集群节点之间有频繁的通信。 Driver 负责任务(tasks)的分发和结果的回收、任务的调度。如果 task的计算结果非常大就不要回收了，会造成 oom。 Worker 是 Standalone 资源调度框架里面资源管理的从节点，也是JVM 进程。 Master 是 Standalone 资源调度框架里面资源管理的主节点，也是JVM 进程。 模型组成&emsp;&emsp;Spark应用程序可分为两部分：Driver部分和Executor部分。 Driver部分 Driver部分主要是对SparkContext进行配置、初始化以及关闭。初始化SparkContext是为了构建Spark应用程序的运行环境，在初始化SparkContext，要先导入一些Spark的类和隐式转换；在Executor部分运行完毕后，需要将SparkContext关闭。 Executor部分 Executor部分是对数据的处理。 数据类型原生数据&emsp;&emsp;包括原生的输入数据和输出数据： 输入数据，Spark目前提供了两种: Scala集合数据集：如Array(1,2,3,4,5)，Spark使用parallelize方法转换成RDD Hadoop数据集：Spark支持存储在hadoop上的文件和hadoop支持的其他文件系统，如本地文件、HBase、SequenceFile和Hadoop的输入格式。例如Spark使用textFile方法可以将本地文件或HDFS文件转换成RDD 输出数据，Spark除了支持上述两种外，还支持scala标量： 生成Scala标量数据，如count（返回RDD中元素的个数）、reduce、fold/aggregate；返回几个标量，如take（返回前几个元素）。 生成Scala集合数据集，如collect（把RDD中的所有元素倒入 Scala集合类型）、lookup（查找对应key的所有值）。 生成hadoop数据集，如saveAsTextFile、saveAsSequenceFile RDD&emsp;&emsp;RDD提供了四种算子： 输入算子：将原生数据转换成RDD，如parallelize、txtFile等。 转换算子：最主要的算子，是Spark生成DAG图的对象，转换算子并不立即执行，在触发行动算子后再提交给driver处理，生成DAG图 –&gt; Stage –&gt; Task –&gt; Worker执行。 缓存算子：对于要多次使用的RDD，可以缓冲加快运行速度，对重要数据可以采用多备份缓存。 行动算子：将运算结果RDD转换成原生数据，如count、reduce、collect、saveAsTextFile等。 共享变量&emsp;&emsp;在Spark运行时，一个函数传递给RDD内的patition操作时，该函数所用到的变量在每个运算节点上都复制并维护了一份，并且各个节点之间不会相互影响。但是在Spark Application中，可能需要共享一些变量，提供Task或驱动程序使用。Spark提供了两种共享变量： 广播变量（Broadcast Variables）：可以缓存到各个节点的共享变量，通常为只读 广播变量缓存到各个节点的内存中，而不是每个 Task。 广播变量被创建后，能在集群中运行的任何函数调用。 广播变量是只读的，不能在被广播后修改。 对于大数据集的广播， Spark 尝试使用高效的广播算法来降低通信成本。 使用方法： 1val broadcastVar ==sc.broadcast(Array(1, 2, 3)) 累加器：只支持加法操作的变量，可以实现计数器和变量求和。用户可以调用SparkContext.accumulator(v)创建一个初始值为v的累加器，而运行在集群上的Task可以使用“+=”操作，但这些任务却不能读取；只有驱动程序才能获取累加器的值。 使用方法： 1234val accum = sc.accumulator(0)；sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum + = x)；accum.value；val num=sc.parallelize(1 to 100)；","categories":[{"name":"学习","slug":"学习","permalink":"http://yoursite.com/categories/学习/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"Spark生态圈","slug":"Spark生态圈","date":"2019-04-26T03:14:44.000Z","updated":"2019-04-26T03:27:25.702Z","comments":true,"path":"2019/04/26/Spark生态圈/","link":"","permalink":"http://yoursite.com/2019/04/26/Spark生态圈/","excerpt":"生态系统&emsp;&emsp;Spark生态圈也被称为BDAS（伯克利数据分析栈），力图在算法（Algorithms）、机器（Machines）、人（People）之间通过大规模集成来展现大数据应用的一个平台。该生态圈已经涉及到了机器学习、数据挖掘、数据库、信息检索、自然语言处理和语音识别等多个领域。 &emsp;&emsp;Spark生态圈以Spark Core为核心，从HDFS、Amazon S3和HBase等持久层读取数据，以YARN和Standalone等作为资源管理器调度Job完成Spark应用程序的计算。这些应用程序可以来自不同的组件，如Spark Shell/Spark Submit的批处理、Spark Streaming的实时处理、Spark SQL的即席查询、BlinkDB的权衡查询、MLlib/MLbase的机器学习、GraphX的图处理和SparkR的数学计算等等。","text":"生态系统&emsp;&emsp;Spark生态圈也被称为BDAS（伯克利数据分析栈），力图在算法（Algorithms）、机器（Machines）、人（People）之间通过大规模集成来展现大数据应用的一个平台。该生态圈已经涉及到了机器学习、数据挖掘、数据库、信息检索、自然语言处理和语音识别等多个领域。 &emsp;&emsp;Spark生态圈以Spark Core为核心，从HDFS、Amazon S3和HBase等持久层读取数据，以YARN和Standalone等作为资源管理器调度Job完成Spark应用程序的计算。这些应用程序可以来自不同的组件，如Spark Shell/Spark Submit的批处理、Spark Streaming的实时处理、Spark SQL的即席查询、BlinkDB的权衡查询、MLlib/MLbase的机器学习、GraphX的图处理和SparkR的数学计算等等。 Spark Core 提供了有向无环图（DAG）的分布式并行计算框架，并提供Cache机制来支持多次迭代计算或者数据共享，大大减少迭代计算之间读取数据局的开销，这对于需要进行多次迭代的数据挖掘和分析性能有很大提升。 在Spark中引入了RDD (Resilient Distributed Dataset) 的抽象，它是分布在一组节点中的只读对象集合，这些集合是弹性的，如果数据集一部分丢失，则可以根据“血统”对它们进行重建，保证了数据的高容错性。 移动计算而非移动数据，RDD Partition可以就近读取分布式文件系统中的数据块到各个节点内存中进行计算。 使用多线程池模型来减少task启动开稍。 采用容错的、高可伸缩性的akka作为通讯框架。 SparkStreaming&emsp;&emsp;SparkStreaming是一个对实时数据流进行高通量、容错处理的流式处理系统，可以对多种数据源（如Kdfka、Flume、Twitter、Zero和TCP 套接字）进行各种复杂算子操作，并将结果保存到外部文件系统、数据库或应用到实时仪表盘。 架构计算流程&emsp;&emsp;SparkStreaming会将计算分解成一系列短小的批处理作业。这里批处理的引擎是Spark Core，SparkStreaming的数据数据会按照batch size分成一段一段的数据，每一段数据都会转换成RDD，这样就将SparkStreaming中对DStream的Transformation操作变成了针对Spark中对RDD的Transformation操作，将RDD经过操作变成的中间结果保存到内存中。 容错性&emsp;&emsp;对于一个流式计算来说，容错性是至关重要的。RDD是一个不可变的分布式可重算的数据集，它记录着操作继承关系（lineage），因此，任意一个RDD的分区出错，都可以利用原始数据重新计算。每一个RDD由多个partition组成，RDD之间通过lineage相连，多个RDD构成了血统，所以当数据是来源于HDFS（多备份）或网络的数据流（SparkStreaming会将网络输入数据的每一个数据流拷贝两份到其他机器）这种能保证容错性的数据源时，RDD的任意partition出错，都可以并行的在其他机器上将缺失的Partition计算出来。这个容错恢复方式比连续计算模型（Storm）的效率更高。 实时性&emsp;&emsp;目前版本的Spark Streaming，其最小的Batch Size的选取在0.5~2秒之间（Storm最小延迟是100ms左右），它是一种准实时的流式处理框架。 扩展性与吞吐量&emsp;&emsp;Spark目前在EC2上已能够线性扩展到100个节点（每个节点4Core），可以以数秒的延迟处理6GB/s的数据量（60M records/s），其吞吐量也比流行的Storm高2～5倍。 Spark SQL前世今生&emsp;&emsp;众所周知，Hive可以将SQL编译成可扩展的MapReduce作业，为了能将Hive和Spark整合，Shark应运而生。Shark就是Hive on Spark，本质上是通过Hive的HQL解析，把HQL翻译成Spark上的RDD操作，然后通过Hive的metadata获取数据库里的表信息，实际HDFS上的数据和文件，会由Shark获取并放到Spark上运算。Shark的最大特性就是快和与Hive的完全兼容，且可以在shell模式下使用rdd2sql()这样的API，把HQL得到的结果集，继续在scala环境下运算，支持自己编写简单的机器学习或简单分析处理函数，对HQL结果进一步分析计算。 &emsp;&emsp;Shark就是Spark SQL的前身，Shark更多是对Hive的改造，替换了Hive的物理执行引擎，因此会有一个很快的速度。然而，不容忽视的是，Shark继承了大量的Hive代码，因此给优化和维护带来了大量的麻烦。随着性能优化和先进分析整合的进一步加深，基于MapReduce设计的部分无疑成为了整个项目的瓶颈。 特点 引入了新的RDD类型SchemaRDD，可以象传统数据库定义表一样来定义SchemaRDD，SchemaRDD由定义了列数据类型的行对象构成。SchemaRDD可以从RDD转换过来，也可以从Parquet文件读入，也可以使用HiveQL从Hive中获取。 内嵌了Catalyst查询优化框架，在把SQL解析成逻辑执行计划之后，利用Catalyst包里的一些类和接口，执行了一些简单的执行计划优化，最后变成RDD的计算。 在应用程序中可以混合使用不同来源的数据，如可以将来自HiveQL的数据和来自SQL的数据进行Join操作。 性能优化 内存列存储（In-Memory Columnar Storage） sparkSQL的表数据在内存中存储不是采用原生态的JVM对象存储方式，而是采用内存列存储； 字节码生成技术（Bytecode Generation） Spark1.1.0在Catalyst模块的expressions增加了codegen模块，使用动态字节码生成技术，对匹配的表达式采用特定的代码动态编译。另外对SQL表达式都作了CG优化， CG优化的实现主要还是依靠Scala2.10的运行时放射机制（runtime reflection）； Scala代码优化 SparkSQL在使用Scala编写代码的时候，尽量避免低效的、容易GC的代码；尽管增加了编写代码的难度，但对于用户来说接口统一。 BlinkDB&emsp;&emsp;BlinkDB 是一个用于在海量数据上运行交互式 SQL 查询的大规模并行查询引擎，它允许用户通过权衡数据精度来提升查询响应时间，其数据的精度被控制在允许的误差范围内。为了达到这个目标，BlinkDB 使用两个核心思想: 一个自适应优化框架，从原始数据随着时间的推移建立并维护一组多维样本； 一个动态样本选择策略，选择一个适当大小的示例基于查询的准确性和（或）响应时间需求。 &emsp;&emsp;和传统关系型数据库不同，BlinkDB是一个很有意思的交互式查询系统，就像一个跷跷板，用户需要在查询精度和查询时间上做一权衡；如果用户想更快地获取查询结果，那么将牺牲查询结果的精度；同样的，用户如果想获取更高精度的查询结果，就需要牺牲查询响应时间。用户可以在查询的时候定义一个失误边界。 MLBase/MLlib&emsp;&emsp;MLBase是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低，让一些可能并不了解机器学习的用户也能方便地使用MLbase。MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。 ML Optimizer会选择它认为最适合的已经在内部实现好了的机器学习算法和相关参数，来处理用户输入的数据，并返回模型或别的帮助分析的结果； MLI 是一个进行特征抽取和高级ML编程抽象的算法实现的API或平台； MLlib是Spark实现一些常见的机器学习算法和实用程序，包括分类、回归、聚类、协同过滤、降维以及底层优化，该算法可以进行可扩充； MLRuntime 基于Spark计算框架，将Spark的分布式计算应用到机器学习领域。 GraphX&emsp;&emsp;GraphX是Spark中用于图(e.g., Web-Graphs and Social Networks)和图并行计算(e.g., PageRank and Collaborative Filtering)的API,可以认为是GraphLab(C++)和Pregel(C++)在Spark(Scala)上的重写及优化，跟其他分布式图计算框架相比，GraphX最大的贡献是，在Spark之上提供一栈式数据解决方案，可以方便且高效地完成图计算的一整套流水作业。GraphX最先是伯克利AMPLAB的一个分布式图计算框架项目，后来整合到Spark中成为一个核心组件。 SparkR&emsp;&emsp;SparkR是AMPLab发布的一个R开发包，使得R摆脱单机运行的命运，可以作为Spark的job运行在集群上，极大得扩展了R的数据处理能力。 &emsp;&emsp;SparkR的几个特性： 提供了Spark中弹性分布式数据集（RDD）的API，用户可以在集群上通过R shell交互性的运行Spark job。 支持序化闭包功能，可以将用户定义函数中所引用到的变量自动序化发送到集群中其他的机器上。 SparkR还可以很容易地调用R开发包，只需要在集群上执行操作前用includePackage读取R开发包就可以了，当然集群上要安装R开发包。 Tachyon&emsp;&emsp;Tachyon是一个高容错的分布式文件系统，允许文件以内存的速度在集群框架中进行可靠的共享，就像Spark和 MapReduce那样。通过利用信息继承，内存侵入，Tachyon获得了高性能。Tachyon工作集文件缓存在内存中，并且让不同的 Jobs/Queries以及框架都能内存的速度来访问缓存文件”。因此，Tachyon可以减少那些需要经常使用的数据集通过访问磁盘来获得的次数。Tachyon兼容Hadoop，现有的Spark和MR程序不需要任何修改而运行。 生态圈应用&emsp;&emsp;Spark生态圈以Spark为核心、以RDD为基础，打造了一个基于内存计算的大数据平台，为人们提供了all-in-one的数据处理方案。人们可以根据不同的场景使用spark生态圈的多个产品来解决应用，而不是使用多个隔离的系统来满足场景需求。下面是几个典型的例子： 场景1：历史数据和实时数据分析查询 通过Spark进行历史数据分析、Spark Streaming进行实时数据分析，最后通过Spark SQL或BlinkDB给用户交互查询。 场景2：欺诈检测、异常行为的发现 通过Spark进行历史数据分析，用MLlib建立数据模型，对Spark Streaming实时数据进行评估，检测并发现异常数据。 场景3：社交网络洞察 通过Spark和GraphX计算社交关系，给出建议。","categories":[{"name":"学习","slug":"学习","permalink":"http://yoursite.com/categories/学习/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"Spark介绍","slug":"Spark介绍","date":"2019-04-25T01:38:51.000Z","updated":"2019-04-26T03:27:34.319Z","comments":true,"path":"2019/04/25/Spark介绍/","link":"","permalink":"http://yoursite.com/2019/04/25/Spark介绍/","excerpt":"Spark简介&emsp;&emsp;Spark是加州大学伯克利分校AMP实验室开发的通用内存并行计算框架。围绕着Spark推出了Spark SQL、Spark Streaming、MLLib、GraphX等组件，也就是BDAS（伯克利数据分析栈），这些组件逐渐形成大数据处理一站式解决平台。 &emsp;&emsp;Spark使用Scala语言实现，Scala 提供一个称为 Actor 的并行模型，其中Actor通过它的收件箱来发送和接收非同步信息而不是共享数据，该方式被称为：Shared Nothing 模型。它具有运行速度快、易用性好、通用性强和随处运行等特点。","text":"Spark简介&emsp;&emsp;Spark是加州大学伯克利分校AMP实验室开发的通用内存并行计算框架。围绕着Spark推出了Spark SQL、Spark Streaming、MLLib、GraphX等组件，也就是BDAS（伯克利数据分析栈），这些组件逐渐形成大数据处理一站式解决平台。 &emsp;&emsp;Spark使用Scala语言实现，Scala 提供一个称为 Actor 的并行模型，其中Actor通过它的收件箱来发送和接收非同步信息而不是共享数据，该方式被称为：Shared Nothing 模型。它具有运行速度快、易用性好、通用性强和随处运行等特点。 Spark和MR的区别 Spark把中间结果放到内存中，迭代运算效率高。MR中计算结果要落地，需要保存到磁盘上，这样会影响整体的速度。 Spark容错性更高，它的弹性分布式数据集RDD可以通过血统进行重建。在RDD计算市，可以通过CheckPoint实现容错。 Spark更加通用，提供了Transformations和Actions两类算子，不像MR只提供了Map和Reduce两种操作。同时，在各个节点间用户可以命名、物化、控制中间结果的存储、分区等。 Spark适用场景大数据处理场景 复杂的批量处理（Batch Data Processing），偏重点在于处理海量数据的能力，至于处理速度可忍受，通常的时间可能是在数十分钟到数小时。 基于历史数据的交互式查询（Interactive Query），通常的时间在数十秒到数十分钟之间。 基于实时数据流的数据处理（Streaming Data Processing），通常在数百毫秒到数秒之间。 Spark场景 Spark是基于内存的迭代计算框架，适用于需要多次操作特定数据集的应用场合。需要反复操作的次数越多，所需读取的数据量越大，受益越大，数据量小但是计算密集度较大的场合，受益就相对较小。 由于RDD的特性，Spark不适用那种异步细粒度更新状态的应用，例如web服务的存储或者是增量的web爬虫和索引。就是对于那种增量修改的应用模型不适合。 数据量不是特别大，但是要求实时统计分析需求。 Spark案例 腾讯 &emsp;&emsp;广点通是最早使用Spark的应用之一。腾讯大数据精准推荐借助Spark快速迭代的优势，围绕“数据+算法+系统”这套技术方案，实现了在“数据实时采集、算法实时训练、系统实时预测”的全流程实时并行高维算法，最终成功应用于广点通pCTR投放系统上，支持每天上百亿的请求量。 &emsp;&emsp;基于日志数据的快速查询系统业务构建于Spark之上的Shark，利用其快速查询以及内存表等优势，承担了日志数据的即席查询工作。在性能方面，普遍比Hive高2-10倍，如果使用内存表的功能，性能将会比Hive快百倍。 Yahoo &emsp;&emsp;Yahoo将Spark用在Audience Expansion中的应用。Audience Expansion是广告中寻找目标用户的一种方法：首先广告者提供一些观看了广告并且购买产品的样本客户，据此进行学习，寻找更多可能转化的用户，对他们定向广告。Yahoo采用的算法是logistic regression。同时由于有些SQL负载需要更高的服务质量，又加入了专门跑Shark的大内存集群，用于取代商业BI/OLAP工具，承担报表/仪表盘和交互式/即席查询，同时与桌面BI工具对接。目前在Yahoo部署的Spark集群有112台节点，9.2TB内存。 淘宝 &emsp;&emsp;阿里搜索和广告业务，最初使用Mahout或者自己写的MR来解决复杂的机器学习，导致效率低而且代码不易维护。淘宝技术团队使用了Spark来解决多次迭代的机器学习算法、高计算复杂度的算法等。将Spark运用于淘宝的推荐相关算法上,同时还利用Graphx解决了许多生产问题，包括以下计算场景：基于度分布的中枢节点发现、基于最大连通图的社区发现、基于三角形计数的关系衡量、基于随机游走的用户属性传播等。 优酷土豆 &emsp;&emsp;优酷土豆在使用Hadoop集群的突出问题主要包括：第一是商业智能BI方面，分析师提交任务之后需要等待很久才得到结果；第二就是大数据量计算，比如进行一些模拟广告投放之时，计算量非常大的同时对效率要求也比较高，最后就是机器学习和图计算的迭代运算也是需要耗费大量资源且速度很慢。 &emsp;&emsp;最终发现这些应用场景并不适合在MapReduce里面去处理。通过对比，发现Spark性能比MapReduce提升很多。首先，交互查询响应快，性能比Hadoop提高若干倍；模拟广告投放计算效率高、延迟小（同hadoop比延迟至少降低一个数量级）；机器学习、图计算等迭代计算，大大减少了网络传输、数据落地等，极大的提高的计算性能。目前Spark已经广泛使用在优酷土豆的视频推荐（图计算）、广告业务等。","categories":[{"name":"学习","slug":"学习","permalink":"http://yoursite.com/categories/学习/"}],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"hexo博客搭建","slug":"hexo博客搭建","date":"2019-04-23T07:25:46.000Z","updated":"2019-04-28T08:19:47.734Z","comments":true,"path":"2019/04/23/hexo博客搭建/","link":"","permalink":"http://yoursite.com/2019/04/23/hexo博客搭建/","excerpt":"简介&emsp;&emsp;最近一段时间比较闲，想着搭个博客玩玩，看了网上主流的博客网站，不是太喜欢，作为一个互联网行业的小渣渣，博客当然要自己搭才有意思了，于是在网上找了一些方案，最终选择了hexo+github的方式来搭建个人博客。","text":"简介&emsp;&emsp;最近一段时间比较闲，想着搭个博客玩玩，看了网上主流的博客网站，不是太喜欢，作为一个互联网行业的小渣渣，博客当然要自己搭才有意思了，于是在网上找了一些方案，最终选择了hexo+github的方式来搭建个人博客。 &emsp;&emsp;使用github pages服务搭建博客的好处有： 全是静态文件，访问速度快； 免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台； 可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的； 数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行； 博客内容可以轻松打包、转移、发布到其它平台； 准备工作&emsp;&emsp;在你的博客之旅开始之前，首先要创建github账号，这个不做过多的介绍。登录你的github账户，创建一个名为你的用户名.github.io的仓库，将来你的博客访问地址就是这个啦。你也可以购买域名替换你的博客地址，当然这是要花钱的。 &emsp;&emsp;仓库建好后，我们需要在电脑上安装git和node.js，在这里要注意Git要提前配置好，和github做绑定，以后要用git工具将代码提交到github上保存的哦。node.js 因为整个博客框架是基于node.js的，所以必须安装node.js环境，安装过程中一路Next即可。 安装hexo框架&emsp;&emsp;准备工作做好后，我们就可以正式开始博客的搭建。Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。&emsp;&emsp;在桌面鼠标右键，选择Git Bash Here，在弹出的Git命令窗口中输入安装命令，然后回车。 1npm install -g hexo-cli ​&emsp;&emsp;选择一个盘创建一个文件夹，在新建的文件夹内鼠标右键，选择Git Bash Here，输入初始化命令，然后回车，等命令执行完，就会看到生成了一系列的文件。 12hexo initnpm install &emsp;&emsp;接着在该文件夹中继续执行以下命令 12hexo ghexo s &emsp;&emsp;命令执行完后浏览器访问http://localhost:4000 或者 127.0.0.1:4000 ,就会看到hexo的初始界面，是不是有着一丝丝的成就感？但是，这个界面还是在本地，别人并不能看到，想要别人看到，我们就必须将这些文件部署到Github上去。 &emsp;&emsp;前面我们已经在github上创建好了博客仓库，接下来我们编辑博客文件夹下的_config.yml文件，在文件最后找到关键字deploy，对其进行编辑，其中repo后面的值要改成你的仓库地址，注意键值对之间要有空格。 1234deploy: type: git repo: https://github.com/ocean233/ocean233.github.io branch: master &emsp;&emsp;保存修改后，如果前面你的git已经可以推送文件到github上的话，你就可以直接执行以下命令将你的博客部署到GitHub上面。 12hexo ghexo d &emsp;&emsp;但是输入hexo d可能会报ERROR Deployer not fount： git错误，这是因为没有安装hexo-deployer-git这个模块，导致Git不能识别该命令，输入下面指令安装该模块即可。 1npm install hexo-deployer-git --save &emsp;&emsp;安装该模块会有些慢，因为Github毕竟是国外的网站，并不是很稳定，所以大家要耐心等待。安装失败时的话大家多试两遍。等模块安装完再次执行hexo d，这时就会有弹出框，输入自己之前注册的github账号进行登录，然后命令行也会要你输入对应的用户名并弹出输入框让你输入密码，填写完毕敲回车即可正确部署。 &emsp;&emsp;在浏览器输入你的用户名.github.io即可看到你自己搭建的博客了哦，如果上面的步骤都没问题，但是没有看到博客的话，可能是有些延迟，大家等等就好。 基础配置&emsp;&emsp;搭建好的博客还很简单，博客样式说实话也是有点丑的，后面我们可以更换博客的主题，让博客更有特色。现在我们先来修改一下博客的基本配置吧。 &emsp;&emsp;对博客的配置修改主要是对配置文件_config.yml进行修改，我们现在的博客还很简单，所以能做的配置并不多，大家可以参考官网上的一些配置信息https://hexo.io/zh-cn/docs/configuration ，我也会列几个主要配置供大家参考。 12345678# Sitetitle: Ocean&apos;s blogsubtitle: 我的目标是星辰大海description: 大数据技术博客keywords:author: Oceanlanguage: zh-CNtimezone: &emsp;&emsp;这里大家可以修改博客的标题信息等，这是大家对博客进行定制化的第一步。 编写文章&emsp;&emsp;我们搭建博客的主要目的自然是为了向大家分享我们的博客内容，绝对不是为了装B，所以如何写一篇文章才是我们应该关注的重点。 &emsp;&emsp;在你的博客文件夹目录下鼠标右键，点击Git Bash Here，接下来命令敲起来，新建一篇文章。如果没有设置 layout 的话，默认使用_config.yml中的 default_layout 参数代替。如果标题包含空格的话，请使用引号括起来。12$ hexo new [layout] &lt;title&gt;$ hexo new &quot;post title with whitespace&quot; &emsp;&emsp;这时候在/博客目录/source/posts目录下可以看到新建的博客文章，以.md结尾，在这里大家可以使用markdown语法编写自己的博客内容。 &emsp;&emsp;博客内容写好后，回到命令行界面，敲命令将我们的博客内容部署到github上。12hexo ghexo d &emsp;&emsp;如果你想先看看编写的博客文章是怎样的，可以会用hexo s命令，在本地浏览器上先查看，没问题了再部署到github上。到这里，博客的基本操作你就已经熟悉了，可以开始玩转hexo了。","categories":[{"name":"博客","slug":"博客","permalink":"http://yoursite.com/categories/博客/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"博客","slug":"博客","permalink":"http://yoursite.com/tags/博客/"}]},{"title":"Linux常用命令","slug":"Linux常用命令","date":"2019-04-22T02:41:35.000Z","updated":"2019-04-26T03:32:06.379Z","comments":true,"path":"2019/04/22/Linux常用命令/","link":"","permalink":"http://yoursite.com/2019/04/22/Linux常用命令/","excerpt":"&emsp;&emsp;最近都在和Linux打交道，我觉得Linux相比windows比较麻烦的就是很多东西都要用命令来控制，当然，这也是很多人喜欢Linux的原因，比较短小但却功能强大。","text":"&emsp;&emsp;最近都在和Linux打交道，我觉得Linux相比windows比较麻烦的就是很多东西都要用命令来控制，当然，这也是很多人喜欢Linux的原因，比较短小但却功能强大。 文件目录1234567mkdir test 创建文件夹rm -rf /test 删除文件夹cd /test 切换文件夹pwd 查看文件夹路径cp -r test /root 拷贝文件夹目录mv test /root 移动文件夹、更改文件夹的名字ls ll 查看文件夹下文件 文件123456789touch test.txt 新建文件cp test.txt newtest.txt 复制文件rm -f test.txt 删除文件cat test.txt 查看文件内容more test.txt 分屏显示文件内容 空格键显示下一页内容，B键显示上一页内容，Q键退出head -10 test.txt 打印文件1-10行tail -10 test.txt 打印最后10行内容tail -f test.txt 实时打印文件内容find 路径 -name test.txt 查找文件或目录，列出路径，可以使用正则表达式查找 vi/vim123456789101112131415161718192021命令行模式 :w保存 :q退出 :q!不保存强制退出 :set nu显示行号 :/单词查找匹配 :N,Md 删除N-M行数据 一般模式： yy复制当前行 nyy复制下面n行 p粘贴到下一行 P粘贴到上一行 G移动到最后一行 nG移动到第n行 n+光标下移n行 n-光标上移n行 H光标移动到屏幕顶行 M光标移动到屏幕中间行 L光标移动到屏幕最后行 dd删除行 x删除光标后一个字符 X删除光标前一个字符 u恢复前一个动作 远程拷贝12scp [-r] test.txt root@node02:`pwd` 本地到远程scp [-r] root@node02:/test /root/ 远程到本地 磁盘指令12df [-m][-k][-h] 查看硬盘信息du [-k][-m][-a][-h][-max-depth=0] /目录 查看目录信息 网络指令12345ifconfig 查看网络配置ping ip地址 查看是否连通netstat 查看网络相关信息telnet 192.168.198.111 22 测试远程主机网络端口 Ctrl+] 输入q退出curl -X GET http://www.baidu.com/ http请求模拟 ​ 系统配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859*用户操作指令： useradd ocean 添加用户，创建一个组 passwd ocean 修改密码 useradd -r ocean 删除用户 usermod -l newocean ocean 修改用户名 usermod -L ocean 锁定账号 usermod -U ocean 解锁账号 /etc/passwd /etc/shodow 查看用户*用户组操作指令： groupadd groupname创建用户组 groupdel groupname删除用户组 groupmod -n newname name 修改用户组名 groups 查看当前登录用户的组内成员 groups ocean 查看指定用户所在组 usermod [-g][-G] 组名 用户 修改用户的主组或者附加组 cat /etc/group 查看组*文件权限： UGO模型：USER GROUP OTHER chown -R user:group 目录名字 修改整个目录下的所有者和属组 chmod ugo+rwx test.txt 修改文件的权限 chmod 700 test.txt 设置权限*系统服务初始化配置： 0：停机状态 1：单用户模式 2：多用户 3：完全多用户 4：为定义 5：图形化 6：停止所有进程，重启*系统时间指令： date 查看时间 date -s 时间 修改时间 时间同步： yum -y install ntp ntpdate cn.ntp.org.cn *配置主机名： vim /etc/sysconfig/network*配置域名映射： vim /etc/hosts*sudo权限配置： vim /etc/sudoers sudo -l*环境变量： vim /etc/profile 全局 echo $path 显示环境变量 source /etc/profile 重新加载环境变量 vi ~/.bash_profile 临时 *防火墙： service iptables status查看状态 chkconfig iptables on/off 永久生效 service iptables start 即时生效 重定向和管道123456789输出重定向：&gt; &gt;&gt;输入重定向：&lt; &lt;&lt;标准输出重定向： 1&gt;错误输出重定向： 2&gt;结合使用：2&gt;&amp;1管道： |命令执行控制：&amp;&amp; 前一个命令执行成功才会执行后一个命令 || 前一个命令执行失败才会执行后一个命令信息黑洞：/dev/null shell脚本123456789101112131415161718定义变量：name=&quot;ocean&quot;引用变量：$name数组：my_array=&#123;A,B,C,D&#125; $&#123;my_array[0]&#125;运算符： 表达式和运算符之间必须有空格 完整的表达式要被 ` ` 包含 val=`expr $a + $b` val=`expr $a - $b` val=`expr $a \\* $b` val=`expr $b / $a` val=`expr $b % $a` $a == $b $a != $b -eq是否相等 -ne是否不相等 -gt左边是否大于右边 -lt左边是否小于右边 -ge左边是大于等于右边 -le左边是否小于等于右边 &amp;&amp; || =字符串是否相等 !=是否不想等 -z长度是否为0 -n长度是否不为0 str是否为不为空 流程控制 函数test()&#123;&#125; 服务指令123456789101112列出所有服务 chkconfig service 服务名 start/stop/status/restart添加服务 /etc/init.d系统各种服务的启动和停止脚本 /etc/rc.d/ 系统对应执行级别的服务软连接步骤：在脚本中添加两行代码#chkconfig: 2345 80 90 #description:auto_run 编写脚本 修改可执行权限 将脚本拷贝到/etc/init.d目录下 加入到服务里chkconfig --add test.sh 重启服务器删除服务：chkconfig --del name服务等级更改：chkconfig --level 2345 name off|on 默认是2345 定时调度1234minute hour day month dayofweek 命令查看定时任务：/var/spool/mail 目录下放各用户定时任务，执行后的信息 /var/spool/cron 目录存放每个用户的定时任务 contab –l 可以直接查看当前用户的定时任务 linux安全1234selinux enforcing强制模式 permissive宽容模式 disabled关闭sestatus -v查看状态 linux进程123456ps -aux查看进程 jobs -lps -ef | grep ssh查看相关进程ps -aux --sort -pcpu根据CPU使用来升序排列top性能分析nohup /root/start.h &amp; 后台运行kill -9 杀死进程 解压压缩下载123456789yum下载wget下载RPM命令：rpm –ivh rpm包 安装 rpm -q ntp 查找 rpm –e 包名 卸载tar命令：tar -zvxf xxxx.tar.gz 解压 tar -zcf 压缩包命名 压缩目标 压缩zip命令：zip -r 包名 目标目录 压缩 unzip filename 解压 ​​​","categories":[{"name":"学习","slug":"学习","permalink":"http://yoursite.com/categories/学习/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://yoursite.com/tags/Linux/"}]}]}